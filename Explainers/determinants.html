<!DOCTYPE html>
<html>
    <head>
        <title>Determinants</title>
        <link rel="stylesheet" href = '../styles.css'>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
    </head>
    <body>
        <a href= '../index.html'>Home</a><br><br>

        <h2>Motivating Determinants</h2>
        <h3>Parallelograms</h3>

        Recall that in 2D, the area of a parallelogram is unchanged by shear transformations. By this, I mean that 
        if \(\boldsymbol{q_1}, \boldsymbol{q_2}\) are orthogonal to each other, then for any \(\beta \in \mathbb R\), the parallelogram with sides 
        \[ 
        \begin{aligned} 
            A & = 0 \\
            B & = \boldsymbol{q_1} + \beta \boldsymbol{q_2} \\ 
            C & = (\boldsymbol{q_1} + \beta \boldsymbol{q_2}) +\boldsymbol{q_2}\\ 
            D & = \boldsymbol{q_2}
        \end{aligned}
        \]
        will have the same area for all \(\beta\). This fact is true in higher dimensions: if you take 
        a parallelogram defined by orthogonal vectors \(\boldsymbol{q}_1 \ldots \boldsymbol{q}_n\), you may freely make the 
        substitutions 
        \[ \boldsymbol{q_i}' = \boldsymbol{q_i} + \sum_{j < i} \beta_{ij} \boldsymbol{q_j} \]
        and leave unchanged the volume defined by the corresponding parallelopiped, for any set of \(\{\beta_{ij} \}_{i \neq j}\). Importantly, you may not change the 
        "original" orthogonal direction: we may add to \(\boldsymbol{q}_i\) only the other orthogonal directions. 
        A consequence of this fact is that if we have some vectors \(\boldsymbol{a_1} \ldots \boldsymbol{a_n}\) and know that for some 
        orthonormal vectors \(\boldsymbol{q}_1 \ldots \boldsymbol{q}_n\) that 
        \[ \begin{aligned} \boldsymbol{a_1} & =  \alpha_1 \boldsymbol{q_1} \\
                           \boldsymbol{a_2} & = \alpha_2 \boldsymbol{q_2} + \beta_{12} \boldsymbol{q_2} \\ 
                           \ldots & \ldots \\ 
                           \boldsymbol{a_i} & = \alpha_i \boldsymbol{q_i} + \sum_{j < i} \beta_{ij} \boldsymbol{q_j},
        \end{aligned} \]
        then the volume of the parallelopiped defined by the \(\boldsymbol{a}_i\)'s is \(\alpha_1 \ldots \alpha_n\).

        <h3>The QR Decomposition</h3>

        The QR Decomposition finds a set of orthogonal directions \(\boldsymbol{q}_1 \ldots \boldsymbol{q}_n\) for which 
        \[ 
        \boldsymbol{a_i} = \alpha_i \boldsymbol{q_i} + \sum_{j < i} \beta_{ij} \boldsymbol{q_j}. \]
        In particular, if we let 
        \[ \begin{aligned} 
        \boldsymbol{A} &= \begin{pmatrix} \boldsymbol{a_1} \ldots \boldsymbol{a_n} \end{pmatrix} \\ 
        \boldsymbol{Q} & = \begin{pmatrix} \boldsymbol{q_1} \ldots \boldsymbol{q_n} \end{pmatrix} \\ 
        \boldsymbol{R} & = \begin{pmatrix} \alpha_1 & \beta_{21} & \beta_{31} & \ldots & \beta_{n1} \\ 
        0 & \alpha_2 & \beta_{32} & \ldots & \beta_{n2} \\ 
        0 & 0 & \alpha_3 & \ldots & \beta_{n3} \\ 
        \ldots & \ldots & \ldots & \ldots & \ldots \\ 
        \ldots & \ldots & \ldots & \ldots & \alpha_n\\ 
        \end{pmatrix},
        \end{aligned} \]
        then one can compactify the expansion of the \(\boldsymbol{a}_i\)s into the equation
        \[ \boldsymbol{A} = \boldsymbol{Q}\boldsymbol{R}. \]
        Moreover, as we have demonstrated, the volume defined by the columns of \(\boldsymbol{A}\) is precisely 
        \(\alpha_1 \ldots \alpha_n\). We shall denote this quantity by \(\text{det}(\boldsymbol{A}) \) and call it the 
        <i>determinant of \(A\).</i> The quantity \(|\text{det}(\boldsymbol{A}) | = |\alpha_1 \ldots \alpha_n|\) is the positive volume 
        defined by the vectors, but we will actually allow the sign to be negative. Technically, I have not shown that this is well defined across different 
        QR decompositions, so have patience.

        <h3>Properties of the Determinant</h3>

        If I were to substitute \(\boldsymbol{a_i}\) with \(\alpha \boldsymbol{a_i}\), then the \(i\)th column of 
        \(\boldsymbol{R}\) would be rescaled by \(\alpha\), so the new determinant would be rescaled by \(\alpha\). 
        What if I were to swap \(\boldsymbol{a_i}\) and \(\boldsymbol{a_{i+1}}\)? Well, the Gram-Schmidt procedure up to index 
        \(i-1\) is left unchanged. Whereas the old Gram-Schmidt procedure would calculate
        \[ 
        \begin{aligned} 
        \boldsymbol{\beta_i} & = \boldsymbol{a_i} - \sum_{j=1}^{i-1} (\boldsymbol{q}_j \cdot \boldsymbol{a_i}) \boldsymbol{q_j} \\ 
        \boldsymbol{q_i} & =\frac{\boldsymbol{\beta_i}}{|\boldsymbol{\beta_i}|} \\ 
        \boldsymbol{\beta_{i+1}} & = \boldsymbol{a_{i+1}} - \sum_{j=1}^{i-1} (\boldsymbol{q}_j \cdot \boldsymbol{a_{i+1}}) \boldsymbol{q_j} - (\boldsymbol{q_i \cdot \boldsymbol{a_{i+1}}}) \boldsymbol{q_i}  \\ 
        \boldsymbol{q_{i+1}} & =\frac{\boldsymbol{\beta_{i+1}}}{|\boldsymbol{\beta_{i+1}}|},
        \end{aligned}\]
        the new procedure would be 
        \[ 
        \begin{aligned} 
        \boldsymbol{\beta_i}' & = \boldsymbol{a_{i+1}} - \sum_{j=1}^{i-1} (\boldsymbol{q}_j \cdot \boldsymbol{a_{i+1}}) \boldsymbol{q_j} \\ 
        \boldsymbol{q_i}' & =\frac{\boldsymbol{\beta_i}}{|\boldsymbol{\beta_i}|} \\ 
        \boldsymbol{\beta_{i+1}}' & = \boldsymbol{a_{i}} - \sum_{j=1}^{i-1} (\boldsymbol{q}_j \cdot \boldsymbol{a_{i}}) \boldsymbol{q_j} - (\boldsymbol{q_i' \cdot \boldsymbol{a_{i}}}) \boldsymbol{q_i}'  \\ 
        \boldsymbol{q_{i+1}}' & =\frac{\boldsymbol{\beta_{i+1}}}{|\boldsymbol{\beta_{i+1}}|}.
        \end{aligned}\]
        After some rearranging, this implies
        \[ \begin{aligned} 
        \boldsymbol{\beta_i}' & =  \boldsymbol{\beta_{i+1}} + \frac{1}{|\boldsymbol{\beta_i}|^2}(\boldsymbol{\beta_i \cdot \boldsymbol{a_{i+1}}}) \boldsymbol{\beta_i} \\ 
        \boldsymbol{\beta_{i+1}}' & = \boldsymbol{\beta_{i}} - \frac{1}{|\boldsymbol{\beta_i'}|^2} (\boldsymbol{\beta_i' \cdot \boldsymbol{a_{i}}}) \boldsymbol{\beta_i}' \\
        \end{aligned}.
        \]

        Therefore, 
        \[ \begin{aligned}
        |\boldsymbol{\beta_i}'|^2 & = |\boldsymbol{\beta_{i+1}}|^2 + \frac{2}{|\boldsymbol{\beta_i}|^2}(\boldsymbol{\beta_i \cdot \boldsymbol{a_{i+1}}}) \boldsymbol{\beta_i} \cdot \boldsymbol{\beta_{i+1}} +  \frac{1}{|\boldsymbol{\beta_i}|^4}(\boldsymbol{\beta_i \cdot \boldsymbol{a_{i+1}}})^2 |\boldsymbol{\beta_i}|^2 \\ 
                                  & = |\boldsymbol{\beta_{i+1}}|^2 + \frac{1}{|\boldsymbol{\beta_i}|^2}(\boldsymbol{\beta_i \cdot \boldsymbol{a_{i+1}}})^2 \\ 
        |\boldsymbol{\beta_{i+1}}'|^2 & = |\boldsymbol{\beta_{i}}|^2 - \frac{2}{|\boldsymbol{\beta_i}'|^2}(\boldsymbol{\beta_i' \cdot \boldsymbol{a_{i}}}) \boldsymbol{\beta_i'} \cdot \boldsymbol{\beta_{i}} +  \frac{1}{|\boldsymbol{\beta_i'}|^4}(\boldsymbol{\beta_i' \cdot \boldsymbol{a_{i}}})^2 |\boldsymbol{\beta_i'}|^2 \\ 
                                  & = |\boldsymbol{\beta_{i}}|^2 + \frac{1}{|\boldsymbol{\beta_i}'|^2}(\boldsymbol{\beta_i' \cdot \boldsymbol{a_{i}}})^2  - \frac{2}{|\boldsymbol{\beta_i}'|^2}(\boldsymbol{\beta_i' \cdot \boldsymbol{a_{i}}}) \boldsymbol{\beta_i'} \cdot \boldsymbol{\beta_{i}}\\                        
        \end{aligned}
        \]
        But \(\boldsymbol{\beta_i'} \cdot \boldsymbol{\beta_i} = \boldsymbol{\beta_i} \cdot \boldsymbol{a_{i+1}} - \sum_{j=1}^{i-1} (\boldsymbol{q}_j \cdot \boldsymbol{a_{i+1}}) (\boldsymbol{\beta_i} \cdot \boldsymbol{q_j}) =   \boldsymbol{\beta_i} \cdot \boldsymbol{a_{i+1}} \), so 
        \[ \begin{aligned}
        |\boldsymbol{\beta_i}'|^2 & = |\boldsymbol{\beta_{i+1}}|^2 + \frac{1}{|\boldsymbol{\beta_i}|^2}(\boldsymbol{\beta_i \cdot \boldsymbol{a_{i+1}}})^2 \\ 
                                  & = |\boldsymbol{\beta_{i+1}}|^2 + (\boldsymbol{q_i} \cdot \boldsymbol{a_{i+1}})^2\\
        |\boldsymbol{\beta_{i+1}}'|^2 & = |\boldsymbol{\beta_{i}}|^2 + \frac{1}{|\boldsymbol{\beta_i}'|^2}(\boldsymbol{\beta_i' \cdot \boldsymbol{a_{i}}})^2 - \frac{2}{|\boldsymbol{\beta_i}'|^2}(\boldsymbol{\beta_i' \cdot \boldsymbol{a_{i}}}) ( \boldsymbol{\beta_i} \cdot \boldsymbol{a_{i+1}}) \\                        
                                 & = |\boldsymbol{\beta_{i}}|^2 + (\boldsymbol{q_i' \cdot \boldsymbol{a_{i}}})^2 - \frac{2}{|\boldsymbol{\beta_i}'|}(\boldsymbol{q_i' \cdot \boldsymbol{a_{i}}}) ( \boldsymbol{\beta_i} \cdot \boldsymbol{a_{i+1}}) 
        \end{aligned}
        \]
        As
        \[ 
        \begin{aligned}
            \alpha_i & = \boldsymbol{q_i} \cdot \boldsymbol{a_i} \\ 
            \alpha_{i+1} & = \boldsymbol{q_{i+1}} \cdot \boldsymbol{a_{i+1}} \\ 
            \alpha_i' & = \boldsymbol{q_i'} \cdot \boldsymbol{a_{i+1}} \\ 
            \alpha_{i+1}' & = \boldsymbol{q_{i+1}'} \cdot \boldsymbol{a_{i}} 
        \end{aligned},
        \]
        this means 
        \[ \begin{aligned}
        |\boldsymbol{\beta_i}'|^2 & = |\boldsymbol{\beta_{i+1}}|^2 + (\boldsymbol{q_i} \cdot \boldsymbol{a_{i+1}})^2\\
        |\boldsymbol{\beta_{i+1}}'|^2 &  = |\boldsymbol{\beta_{i}}|^2 + (\boldsymbol{q_i' \cdot \boldsymbol{a_{i}}})^2 - \frac{2}{|\boldsymbol{\beta_i}'|}(\boldsymbol{q_i' \cdot \boldsymbol{a_{i}}}) ( \boldsymbol{\beta_i} \cdot \boldsymbol{a_{i+1}}) 
        \end{aligned}
        \]


    </body>
</html>