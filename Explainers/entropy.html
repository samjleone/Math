<!DOCTYPE html>
<html>
    <head>
        <title>Entropy</title>
        <link rel="stylesheet" href = '../styles.css'>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
    </head>
    <body>
        <a href= '../index.html'>Home</a><br><br>

        <h3>Deriving Entropy</h3>

        Given a probability measure with expected value \(\mathbb E\) and Probability Mass Function \(p\), one typically defines 
        the entropy as 
        \[ H(p) = \mathbb E\bigg[ \log\bigg(\frac{1}{p(X)}\bigg) \bigg] \]
        the idea being that \(\log\big(\frac{1}{p(X)}\big)\) is a measure of how "uncertain' the event \(X\) is. For example, if the probability 
        is \(p = 2^{-10}\), it's "unlikelinesss" is 10. One could of course accept this definition, and upon defining the other information theoretic 
        quantities, it becomes clear that it is an appropriate one. But how could one <b>arrive at this expression?</b>

        <h4>Approach 1: In Terms of Average Probability</h4>

        Imagine that you flip an unfair coin 1,000 times, and suppose the probability of heads is 0.6. You would expect that, 
        for the most part, you would have 600 heads and 400 tails, give or take. You do not expect the number of heads to 
        be uniform. Is it possible to formalize this intuition? Departing from this specific case, you might think about  
        \(X_1 \ldots X_n\) independent draws from a particular probability distribution. For a given draw, the joint probability is 
        \[ p(X_1)p(X_2)\ldots p(X_n). \]
        As you take \(n\) to be very large, what's the limit? Well, zero, unless your distribution is totally deterministic (see Borel-Cantelli II). 
        Assuming we want to keep things interesting, we might be interested in the normalized limit: 
        \[ \bigg(p(X_1)p(X_2)\ldots p(X_n)\bigg)^{1/n} \to \mu \]
        We don't yet know what \(\mu\) is, but we assume it exists! This expression is complicated. But we recognize that this is almost something that the law of large numbers woul have to 
        apply to, so long as we take the appropriate log transformation:
        \[ \frac{1}{n} \sum_{i=1}^n \log\bigg( p(X_i) \bigg) \to \log \mu \]
        And we know from the law of large numbers that, necessarily, this limit has to be the expected value of \(\log\bigg( p(X_i) \bigg)\). And thus, 
        \[ \log \mu = \mathbb E\bigg[ \log\bigg(p(X) \bigg) \bigg] \implies \mu = 2^{-\mathbb E\big[ \log\big(\frac{1}{p(X)}\big) \big]} \]
        In other words, a typical set of trials has typical probability close to 
        \[ p(X_1,X_2 \ldots X_n) = 2^{- n \mathbb E\big[ \log\big(\frac{1}{p(X)}\big) \big]}.  \]
        Which is exponentially small in \(n\). But the coefficient attached to the \(n\) says how quickly the likelihood should decay. 
        If the quantity is large, you'd expect that everything becomes unlikely very quickly. If it is small, you'd imagine that 
        things can remain somewhat likely. In the limiting case, if it is zero, then the right hand side is 1, meaning everything is 
        is deterministic.

        <h4>Approach 2: In Terms of Random Number Generation</h4>

        You might take the constructive approach. Suppose you want to draw a sample from the given distribution. You have a random number generator 
        at your disposal, which can churn out a random sequence of 0's and 1's, each bit independently and with probability 1/2. For example, if 
        you want to roll a "4-sided die" with sides A,B,C,D, you might write down the rules 
        \[ 
        \begin{aligned} 
            00 \mapsto A\\
            01 \mapsto B \\
            10 \mapsto C \\
            11 \mapsto D\\
        \end{aligned}
        \]
        In any case, you need two random bits to sample. You might then say that the distribution intrinsically has two bits of information. 
        What about other distributions? By our logic, any uniform distribution over \(2^n\) number would have \(n\) bits of information, but what about 
        more generally? <br><br> 

        If our probabilities are not dyadic (a multiple of some power of two), then any rule would require infinitely many bits, and that won't do. 
        To simplify things, we'll argue about distributions whose probabilities are all dyadic. So then suppose that we have probabilities \(p_1 \ldots p_n\) which 
        are all dyadic. Then there will exist some "maximum resolution" integer \(M\) for which all probabilities are multiple of \(2^{-M}\). In other words, 
        there exist constants \(b_n\) such that \(p_n = b_n 2^{-M}\). Without loss of generality, 
        assume that our probabilities are labeled in descending order of magnitude: 
        \[ p_1 \geq p_2 \geq \ldots p_n \]
        You can imagine that these probabilities partition \([0,1]\) into \(n\) non-overlapping segments \(s_1 \ldots s_n\), each of length \(p_1,p_2\ldots p_n\):
        \[ s_j = \bigg[\sum_{k < j}p_k, \sum_{k < j}p_k + p_j\bigg) \]
        Given this, a straightforward way of drawing from the distribution would be to draw a random number \(U\) from the uniform distribution on \([0,1]\), and 
        then whichever \(s_n\) that number lands in will be the outcome we return, which will work because the probability of a random number landing in the interval 
        \(s_n\) is proportional to its length, which is \(p_n\). While a random number in \([0,1]\) might feel like a big ask, we don't actually need a random <i>real number</i>, just 
        a random multiple of \(2^{-M}\). <br><br>

        With this algorithm, we would associate each outcome to \(b_n\) of these random bit sequences. For instance, if \(p_1\) were 1/2, we could associate it to all the bit sequences 
        starting with zero, and if \(p_2\) were then a quarter, we could associate it to all those whose first bit is \(1\) but whose second bit is zero, and so on.
        Notice that, in this process, quite a few of the characters go to waste! In our example, the first digit was sufficient to let us know we're in \(s_1\), the first two digits 
        are necessary to let us know we're in \(s_2\). Evidently, there are only \(\log(1/p_i)\) "useful bits" in this encoding scheme. Therefore, we would say that 
        this is the intrinsic amount of information contained in an event probability \(p_i\). And then the average likelihood is, 
        \[ \mathbb E\bigg[ \log\bigg(\frac{1}{p(X)}\bigg)  \bigg], \]
        which we recognize as the entropy. In this discussion, it was not proved that this association algorithm is "optimal." In fact, it is true that 
        this algorithm, known as "Shannon Coding," is optimal in the expected number of bits, at least when the powers are dyadic. As we have achieved 
        an expected code length coinciding with entropy, to show optimality, it suffices to show that it's impossible to do better. I will not prove this here formally, 
        partly because it requires additional machinery and rules on the kinds of algorithms which are allowed. The main rule is that the code be a "prefix" code. 
        What does that mean? In our earlier discussion, we argued that an event of probability 1/2 because we can detect this outcome using only the first bit of information. 
        This all only works if, for a given string representing an outcome, there is no other second code for some other outcome entirely containing the first code, since then 
        we would have to look further. This requirement means that any good coding rule is a "prefix code." And the reason our code is optimal is because it smushed the shortest 
        codes with the most likely events.

    </body>
</html>